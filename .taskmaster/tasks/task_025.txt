# Task ID: 25
# Title: Implement CapitalOne CSV Transaction Parser
# Status: done
# Dependencies: 2, 3, 20
# Priority: medium
# Description: Create a parser module that can read and process CapitalOne CSV transaction downloads, converting them to a standardized DataFrame format compatible with the existing parser system.
# Details:
Create a new parser module in `dataextractai/parsers/capitalone_csv_parser.py` with the following implementation:

```python
import pandas as pd
from pathlib import Path
from typing import Union, IO, Dict, Any, Optional

from dataextractai.parsers_core.base import BaseParser

class CapitalOneCSVParser(BaseParser):
    """Parser for CapitalOne CSV transaction downloads."""
    
    @property
    def name(self) -> str:
        """Return the unique name of this parser."""
        return "capitalone_csv"
        
    @property
    def supported_file_types(self) -> list:
        """Return list of file extensions this parser supports."""
        return [".csv"]
    
    def can_parse(self, file_path: Union[str, Path]) -> bool:
        """
        Determine if this parser can handle the given file.
        
        Args:
            file_path: Path to the file to check
            
        Returns:
            True if this parser can handle the file, False otherwise
        """
        if not str(file_path).lower().endswith('.csv'):
            return False
            
        # Try to read the first few lines to check if it matches CapitalOne format
        try:
            with open(file_path, 'r') as f:
                header = f.readline().strip()
                # Check for characteristic CapitalOne CSV headers
                return any(marker in header for marker in [
                    "Transaction Date", 
                    "Posted Date", 
                    "Card No.", 
                    "Description", 
                    "Category"
                ])
        except Exception:
            return False
    
    def parse_file(self, file_path: Union[str, Path, IO]) -> Dict[str, Any]:
        """
        Parse the given CapitalOne CSV file.
        
        Args:
            file_path: Path to the file to parse or file-like object
            
        Returns:
            Dictionary containing parsed data with 'transactions' key containing
            a DataFrame of standardized transaction data
        """
        # Read the CSV file
        df = pd.read_csv(file_path)
        
        # Map CapitalOne CSV columns to our standardized format
        # Note: Column names may vary slightly between different CapitalOne products
        column_mapping = {
            'Transaction Date': 'transaction_date',
            'Posted Date': 'posted_date',
            'Description': 'description',
            'Category': 'category',
            'Debit': 'amount',
            'Credit': 'amount_credit',
            'Card No.': 'card_number'
        }
        
        # Handle variations in column names
        actual_mapping = {}
        for cap_col, std_col in column_mapping.items():
            for col in df.columns:
                if cap_col.lower() in col.lower():
                    actual_mapping[col] = std_col
                    break
        
        # Rename columns based on the mapping
        df = df.rename(columns=actual_mapping)
        
        # Ensure required columns exist
        required_columns = ['transaction_date', 'description', 'amount']
        for col in required_columns:
            if col not in df.columns:
                if col == 'amount' and 'amount_credit' in df.columns:
                    # Handle case where only credit column exists
                    df['amount'] = -df['amount_credit']
                else:
                    df[col] = None
        
        # Combine debit and credit columns if both exist
        if 'amount' in df.columns and 'amount_credit' in df.columns:
            # Convert to numeric, handling any formatting issues
            df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
            df['amount_credit'] = pd.to_numeric(df['amount_credit'], errors='coerce')
            
            # Fill NaN values with 0
            df['amount'] = df['amount'].fillna(0)
            df['amount_credit'] = df['amount_credit'].fillna(0)
            
            # Credit amounts are positive in CapitalOne CSVs, but we want them negative
            # Debit amounts are already positive
            df['amount'] = df['amount'] - df['amount_credit']
            df = df.drop(columns=['amount_credit'])
        
        # Standardize date format
        df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')
        if 'posted_date' in df.columns:
            df['posted_date'] = pd.to_datetime(df['posted_date'], errors='coerce')
        
        # Add source information
        if isinstance(file_path, (str, Path)):
            df['source_file'] = str(file_path)
        else:
            df['source_file'] = 'file_object'  # For file-like objects
            
        df['source'] = 'CapitalOne'
        
        # Add transaction_type based on amount
        df['transaction_type'] = df['amount'].apply(lambda x: 'credit' if x < 0 else 'debit')
        
        # Return the standardized DataFrame
        return {
            'transactions': df,
            'metadata': {
                'source': 'CapitalOne CSV',
                'parser': self.name
            }
        }
```

Next, register the parser in the parser registry by updating `dataextractai/parsers/__init__.py`:

```python
# Import all parsers here to ensure they are registered
from .capitalone_csv_parser import CapitalOneCSVParser
# ... other parser imports ...

# This ensures all parsers are registered when the package is imported
```

Ensure the parser is properly integrated with the existing system by:

1. Following the BaseParser interface contract
2. Implementing all required methods
3. Handling variations in CapitalOne CSV formats (different card products may have slightly different formats)
4. Properly mapping columns to the standardized schema
5. Converting date formats and handling amount fields correctly (especially credit vs debit)
6. Registering the parser in the parser registry

The parser should be able to detect CapitalOne CSV files based on their structure and handle the conversion to the standardized format used throughout the system.

Key standardized output fields include:
- transaction_date: Parsed to standard datetime format
- description: Transaction description from CapitalOne
- amount: Combined amount (debits positive, credits negative)
- source_file: Path to the source file
- source: Set to 'CapitalOne'
- transaction_type: 'debit' or 'credit' based on amount
- category: Original category from CapitalOne
- card_number: Card number from the CSV (included as metadata)

# Test Strategy:
To verify the CapitalOne CSV parser implementation:

1. Create unit tests in `tests/parsers/test_capitalone_csv_parser.py`:
   ```python
   import pytest
   import pandas as pd
   from pathlib import Path
   import io
   from dataextractai.parsers.capitalone_csv_parser import CapitalOneCSVParser
   from dataextractai.parsers_core.registry import ParserRegistry

   @pytest.fixture
   def sample_csv_content():
       return """Transaction Date,Posted Date,Card No.,Description,Category,Debit,Credit
   2024-01-15,2024-01-16,1234,GROCERY STORE,Groceries,45.67,
   2024-01-17,2024-01-18,1234,REFUND: ONLINE STORE,Merchandise,,15.99
   2024-01-20,2024-01-21,1234,GAS STATION,Gas/Automotive,32.50,
   """

   @pytest.fixture
   def sample_csv_file(sample_csv_content, tmp_path):
       file_path = tmp_path / "2024_Capital_one_transaction_download.csv"
       with open(file_path, "w") as f:
           f.write(sample_csv_content)
       return file_path

   def test_parser_registration():
       # Test that the parser is properly registered
       parser = ParserRegistry.get_parser_by_name("capitalone_csv")
       assert parser is not None
       assert parser().name == "capitalone_csv"
       
   def test_can_parse(sample_csv_file):
       parser = CapitalOneCSVParser()
       assert parser.can_parse(sample_csv_file) is True
       assert parser.can_parse("not_a_csv_file.txt") is False
       
   def test_parse_file(sample_csv_file, sample_csv_content):
       # Test parsing from file path
       parser = CapitalOneCSVParser()
       result = parser.parse_file(sample_csv_file)
       
       assert 'transactions' in result
       assert isinstance(result['transactions'], pd.DataFrame)
       assert len(result['transactions']) == 3
       
       # Test required columns exist
       assert all(col in result['transactions'].columns for col in [
           'transaction_date', 'description', 'amount', 'source', 'source_file', 'transaction_type'
       ])
       
       # Test amount calculation (debit positive, credit negative)
       assert result['transactions'].iloc[0]['amount'] == 45.67
       assert result['transactions'].iloc[1]['amount'] == -15.99
       assert result['transactions'].iloc[0]['transaction_type'] == 'debit'
       assert result['transactions'].iloc[1]['transaction_type'] == 'credit'
       
       # Test parsing from file-like object
       file_obj = io.StringIO(sample_csv_content)
       result2 = parser.parse_file(file_obj)
       assert len(result2['transactions']) == 3
   ```

2. Create integration tests to verify the parser works with the CLI:
   ```python
   import pytest
   import subprocess
   import os
   import pandas as pd
   from pathlib import Path

   def test_cli_with_capitalone_csv(sample_csv_file, tmp_path):
       output_file = tmp_path / "output.csv"
       
       # Run the CLI command
       result = subprocess.run([
           "python", "-m", "dataextractai.cli",
           "parse",
           "--input", str(sample_csv_file),
           "--output", str(output_file)
       ], capture_output=True, text=True)
       
       # Check command succeeded
       assert result.returncode == 0
       
       # Verify output file exists and has correct content
       assert output_file.exists()
       df = pd.read_csv(output_file)
       assert len(df) == 3
       assert all(col in df.columns for col in [
           'transaction_date', 'description', 'amount', 'source', 'transaction_type'
       ])
   ```

3. Add edge case tests:
   ```python
   @pytest.fixture
   def malformed_csv_content():
       return """Transaction Date,Posted Date,Card No.,Description,Category,Debit,Credit
   2024-01-15,2024-01-16,1234,GROCERY STORE,Groceries,invalid,
   2024-01-17,2024-01-18,1234,REFUND: ONLINE STORE,Merchandise,,
   2024-01-20,,1234,GAS STATION,Gas/Automotive,32.50,
   """

   @pytest.fixture
   def different_headers_csv_content():
       return """Trans Date,Post Date,Card Number,Transaction Description,Trans Category,Debit Amount,Credit Amount
   2024-01-15,2024-01-16,1234,GROCERY STORE,Groceries,45.67,
   2024-01-17,2024-01-18,1234,REFUND: ONLINE STORE,Merchandise,,15.99
   """

   def test_malformed_data_handling(malformed_csv_content, tmp_path):
       file_path = tmp_path / "malformed.csv"
       with open(file_path, "w") as f:
           f.write(malformed_csv_content)
           
       parser = CapitalOneCSVParser()
       result = parser.parse_file(file_path)
       
       # Should still parse but handle invalid data gracefully
       assert len(result['transactions']) == 3
       # First row should have NaN for amount due to 'invalid'
       assert pd.isna(result['transactions'].iloc[0]['amount'])
       # Second row should have 0 for amount (missing credit/debit)
       assert result['transactions'].iloc[1]['amount'] == 0
       # Third row should have missing posted_date but valid amount
       assert result['transactions'].iloc[2]['amount'] == 32.50
       assert pd.isna(result['transactions'].iloc[2]['posted_date'])

   def test_different_headers(different_headers_csv_content, tmp_path):
       file_path = tmp_path / "different_headers.csv"
       with open(file_path, "w") as f:
           f.write(different_headers_csv_content)
           
       parser = CapitalOneCSVParser()
       # Should still detect as CapitalOne CSV despite different headers
       assert parser.can_parse(file_path) is True
       
       result = parser.parse_file(file_path)
       # Should map columns correctly despite different names
       assert len(result['transactions']) == 2
       assert all(col in result['transactions'].columns for col in [
           'transaction_date', 'description', 'amount'
       ])
   ```

4. Manual testing:
   - Obtain a real CapitalOne CSV transaction download (or create a realistic mock)
   - Run the parser through the CLI: `python -m dataextractai.cli parse --input 2024_Capital_one_transaction_download.csv --output parsed_transactions.csv`
   - Verify the output CSV contains all transactions with correct amounts, dates, and descriptions
   - Check that credit transactions have negative amounts and debit transactions have positive amounts
   - Verify that all standardized fields are present: transaction_date, description, amount, source_file, source, transaction_type
   - Confirm that card_number and category are preserved from the original data

# Subtasks:
## 25.1. Scaffold parser module and register it [done]
### Dependencies: None
### Description: Create the basic structure of the CapitalOneCSVParser class and register it in the parser registry.
### Details:


## 25.2. Implement parse_file logic for column mapping and normalization [done]
### Dependencies: None
### Description: Implement the core parsing logic to map CapitalOne CSV columns to the standardized schema, normalize amounts (debits positive, credits negative), and standardize dates.
### Details:


## 25.3. Implement can_parse logic for CapitalOne CSV detection [done]
### Dependencies: None
### Description: Create robust detection logic to identify CapitalOne CSV files based on their headers, handling variations in column names.
### Details:


## 25.4. Write unit tests for parser [done]
### Dependencies: None
### Description: Create comprehensive unit tests for the parser, including tests for normal operation, edge cases, and error handling.
### Details:


## 25.5. Write integration tests for CLI/standalone use [done]
### Dependencies: None
### Description: Create integration tests to verify the parser works correctly when used through the CLI or as a standalone module.
### Details:


## 25.6. Document parser usage and edge cases [done]
### Dependencies: None
### Description: Add comprehensive docstrings and comments to the parser code, documenting its usage, limitations, and how it handles edge cases.
### Details:


## 25.7. Validate output with provided sample and real data [done]
### Dependencies: None
### Description: Test the parser with both sample data and real CapitalOne CSV exports to ensure it correctly handles all variations and produces the expected standardized output.
### Details:


## 26.7. Scaffold parser module and register it [done]
### Dependencies: None
### Description: Create capitalone_csv_parser.py in dataextractai/parsers/, inherit from BaseParser, and register in the parser registry.
### Details:


## 27.7. Implement parse_file logic for column mapping and normalization [done]
### Dependencies: None
### Description: Implement parse_file to read CSV, map/normalize columns, combine debit/credit, standardize dates, and output required fields.
### Details:


## 28.7. Implement can_parse logic for CapitalOne CSV detection [done]
### Dependencies: None
### Description: Implement can_parse to detect CapitalOne CSVs by header (e.g., Transaction Date, Debit, Credit, etc.).
### Details:


## 29.7. Write unit tests for parser [done]
### Dependencies: None
### Description: Write unit tests to verify correct parsing, normalization, and error handling for edge cases.
### Details:


## 30.7. Write integration tests for CLI/standalone use [done]
### Dependencies: None
### Description: Write integration tests to ensure the parser works with CLI and modular workflows.
### Details:


## 31.7. Document parser usage and edge cases [done]
### Dependencies: None
### Description: Document how to use the parser, expected input/output, and known edge cases.
### Details:


## 32.7. Validate output with provided sample and real data [done]
### Dependencies: None
### Description: Test the parser with the provided CapitalOne CSV sample and real data to ensure correct output.
### Details:


