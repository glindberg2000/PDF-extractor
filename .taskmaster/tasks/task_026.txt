# Task ID: 26
# Title: Migrate Legacy Financial Statement Parsers to Modular Format
# Status: pending
# Dependencies: 2, 3, 5, 17
# Priority: medium
# Description: Refactor remaining legacy financial statement parsers to use the modular format, implementing strict auto-detection and robust extract_metadata methods following established patterns.
# Details:
This task involves migrating the remaining legacy financial statement parsers to the new modular format. For each parser:

1. Create a new class that inherits from BaseParser in the appropriate location within the parsers/ directory
2. Implement all required methods from the BaseParser interface:
   - `name` property: Return a unique identifier for the parser
   - `can_parse` method: Implement strict auto-detection logic that checks for specific patterns in the file
   - `parse` method: Extract transactions and metadata from the file
   - `extract_metadata` method: Extract account numbers, statement dates, and other relevant metadata

3. Follow the production-grade patterns established in the Chase Checking, First Republic Bank, and Wells Fargo Visa parsers:
   - Use regular expressions for reliable pattern matching
   - Implement robust error handling with informative error messages
   - Add detailed logging for debugging purposes
   - Include comprehensive docstrings for all methods

4. For each parser, document the extraction logic in the Memory Bank, including:
   - Key patterns used for auto-detection
   - Statement structure and format details
   - Transaction table format and parsing approach
   - Metadata extraction patterns and locations

Example implementation structure:
```python
from ..base import BaseParser
from ..registry import ParserRegistry
from typing import Dict, Any, Union, Optional, IO
from pathlib import Path
import re
import pandas as pd
from datetime import datetime

@ParserRegistry.register
class BankOfAmericaCheckingParser(BaseParser):
    """Parser for Bank of America checking account statements."""
    
    @property
    def name(self) -> str:
        return "bankofamerica_checking"
        
    def can_parse(self, file_path: Union[str, Path], file_obj: Optional[IO] = None) -> bool:
        """Determine if this parser can parse the given file."""
        # Implement strict auto-detection logic
        text_content = self._extract_text(file_path, file_obj)
        return bool(re.search(r"Bank of America", text_content, re.IGNORECASE) and 
                   re.search(r"Account Statement", text_content, re.IGNORECASE))
    
    def parse(self, file_path: Union[str, Path], file_obj: Optional[IO] = None) -> pd.DataFrame:
        """Parse the statement file and return a DataFrame of transactions."""
        text_content = self._extract_text(file_path, file_obj)
        metadata = self.extract_metadata(text_content)
        transactions = self._extract_transactions(text_content)
        
        # Add metadata to each transaction
        for key, value in metadata.items():
            transactions[key] = value
            
        return transactions
    
    def extract_metadata(self, text_content: str) -> Dict[str, Any]:
        """Extract metadata from the statement."""
        metadata = {}
        
        # Extract account number
        account_match = re.search(r"Account Number:\s*(\d+)", text_content)
        if account_match:
            metadata["account_number"] = account_match.group(1)
            
        # Extract statement period
        period_match = re.search(r"Statement Period:\s*(\w+\s+\d{1,2},\s*\d{4})\s+through\s+(\w+\s+\d{1,2},\s*\d{4})", text_content)
        if period_match:
            metadata["statement_start_date"] = period_match.group(1)
            metadata["statement_end_date"] = period_match.group(2)
            
        return metadata
        
    def _extract_transactions(self, text_content: str) -> pd.DataFrame:
        """Extract transaction data from the text content."""
        # Implementation details for transaction extraction
        # ...
```

Repeat this process for each remaining legacy parser that needs to be migrated.

# Test Strategy:
To verify the correct implementation of each migrated parser:

1. **Unit Testing**:
   - Create unit tests for each parser class that verify:
     - The `can_parse` method correctly identifies valid statements and rejects invalid ones
     - The `extract_metadata` method extracts all expected metadata fields
     - The `parse` method correctly extracts transactions with proper formatting
   - Use pytest parametrization to test with multiple sample files

2. **Integration Testing**:
   - Test each parser through the registry system to ensure it's properly registered
   - Verify the parser can be retrieved by name and file type
   - Test the parser through the CLI interface to ensure it works end-to-end

3. **Real-World Testing**:
   - Test each parser with a set of real statement files (with sensitive data redacted)
   - Verify that all transactions are correctly extracted with proper dates, descriptions, and amounts
   - Compare the output with manually extracted data to ensure accuracy
   - Test edge cases like statements with no transactions, unusual formatting, etc.

4. **Documentation Verification**:
   - Review the Memory Bank documentation for each parser to ensure it includes:
     - Clear description of the statement format
     - Examples of patterns used for detection and extraction
     - Notes on any special handling or edge cases
     - Sample input/output (with sensitive data removed)

5. **Backward Compatibility**:
   - Verify that existing workflows using these parsers continue to work
   - Check that the output format is compatible with downstream systems
   - Ensure CLI commands produce the same results as before

Document all test results, including any issues found and how they were resolved.
